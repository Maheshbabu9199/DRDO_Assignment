{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Questions 05\n",
        "\n",
        "A military organization wants to develop a machine learning model that can identify enemy\n",
        "vehicles in satellite imagery. The model will take as input a satellite image and output a list\n",
        "of bounding boxes that correspond to the location of enemy vehicles in the image. Develop\n",
        "a ML solution for the aforesaid scenario with an example Dataset."
      ],
      "metadata": {
        "id": "Ii01mYNnnpvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implemeting the necessary libraries"
      ],
      "metadata": {
        "id": "bg_zTPWVnY_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AF0DU9zpc07",
        "outputId": "20e7d5a0-e01a-4c24-e6a3-35341b408b15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/645.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/645.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m553.0/645.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.2/645.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3nSU375tO-ZO"
      },
      "outputs": [],
      "source": [
        "# For data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For saving the model\n",
        "import pickle\n",
        "\n",
        "# For warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# importing the pre-trained YOLO model class\n",
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here in this cell, we have taken the pre-trained yolov8 model.\n",
        "\n",
        "There are many versions of YOLO models available but the factors that make the difference in each and every version is the speed,accuracy and parameters and mAP (mean average precision value)."
      ],
      "metadata": {
        "id": "Vy3ppgFVbbvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About the dataset\n",
        "\n",
        "\n",
        "Dataset Structure\n",
        "Open Images V7 is structured in multiple components catering to varied computer vision challenges:\n",
        "\n",
        "Images: About 9 million images, often showcasing intricate scenes with an average of 8.3 objects per image.\n",
        "Bounding Boxes: Over 16 million boxes that demarcate objects across 600 categories.\n",
        "Segmentation Masks: These detail the exact boundary of 2.8M objects across 350 classes.\n",
        "Visual Relationships: 3.3M annotations indicating object relationships, properties, and actions.\n",
        "Localized Narratives: 675k descriptions combining voice, text, and mouse traces.\n",
        "Point-Level Labels: 66.4M labels across 1.4M images, suitable for zero/few-shot semantic segmentation."
      ],
      "metadata": {
        "id": "HWN2Knq6lpzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the pre-trained model\n",
        "\n",
        "base_model = YOLO(\"yolov8n.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao4UPk-OT3X3",
        "outputId": "e7263bcf-a01e-425b-8e55-25d9c2288605"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 194MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions and storing in results variable\n",
        "\n",
        "results = base_model.predict(\"/content/pexels-gül-işık-18984676.jpg\", save=True, classes=[1,2,3,5,7])\n",
        "\n",
        "# we have used only specific classes like [bicycles, cars, motorcycle, trucks] to display the bounding boxes"
      ],
      "metadata": {
        "id": "97EloUIGVu58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd48cdb5-7d10-44a8-f6ba-f67e261d004e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/pexels-gül-işık-18984676.jpg: 640x448 2 bicycles, 2 cars, 1 motorcycle, 156.5ms\n",
            "Speed: 4.4ms preprocess, 156.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the co-ordinates xyxy\n",
        "\n",
        "for result in results:\n",
        "  print(result.boxes.xyxy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfRmPVxYq8pm",
        "outputId": "9c78cbd5-17e1-4a6a-f317-d2ce26e09a3b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 816.8940, 4492.8984, 2643.5737, 6294.9946],\n",
            "        [2240.9717, 4190.5327, 3001.8154, 4675.4028],\n",
            "        [   0.0000, 4101.8276, 1081.4702, 4951.4741],\n",
            "        [4323.6787, 4360.7671, 4589.0752, 5135.5498],\n",
            "        [3110.1943, 4632.0186, 3538.6572, 5682.8394]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the co-ordinates xywh\n",
        "\n",
        "for result in results:\n",
        "  print(result.boxes.xywh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8yE4zSJ6wzz",
        "outputId": "da788669-b924-4be5-a085-5b2671166f83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1730.2339, 5393.9463, 1826.6797, 1802.0962],\n",
            "        [2621.3936, 4432.9678,  760.8438,  484.8701],\n",
            "        [ 540.7351, 4526.6509, 1081.4702,  849.6465],\n",
            "        [4456.3770, 4748.1582,  265.3965,  774.7827],\n",
            "        [3324.4258, 5157.4287,  428.4629, 1050.8208]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In the scenario, we have used the YOLO algorithms\n",
        "\n",
        "YOLO (You look only Once) algorithm looks at the images only once and will be able to predict the objects inside the images.\n",
        "\n",
        "This works on the concept of taking the images and forming the bounding rectangle boxes vertically and horizontally.\n",
        "\n",
        "Once the images are formed this will take the most common area of the two bounding boxes as moving from left to right.\n",
        "The algorithm used to filter the bounding boxes based on their weightage is non-maximum suppression(NMS).\n",
        "\n",
        "This is used for sorting out the algorithms with more weightage and remove the remaining both in rectangle and horizontal boxes.\n",
        "\n",
        "Once the area is figured out, it then classifies whether the area belongs to which class.\n",
        "\n",
        "\n",
        "Here, we are listing the bounding boxes that contains 4 values.\n",
        "\n",
        "\n",
        "xywh:\n",
        "x_center = (box_x_left+box_x_width/2)/image_width\n",
        "y_center = (box_y_top+box_height/2)/image_height\n",
        "width = box_width/image_width\n",
        "height = box_height/image_height\n",
        "\n",
        "\n",
        "We can take all the predictions in the results variables which is of ultralytics boxes type and then by iterating through this variable, we can get each and every image prediction details.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FE3R1OQRbbpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze -> requirements.txt"
      ],
      "metadata": {
        "id": "bCUOx5ycoLlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bd2fIUPgoNvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}